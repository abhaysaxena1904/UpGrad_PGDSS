{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Images\n",
    "\n",
    "In this notebook, we will go through the end-to-end pipeline of training conv nets, i.e. organising the data into directories, preprocessing, data augmentation, model building etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat '../../../kaggle.json': No such file or directory\n",
      "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/disks/user/anaconda3/bin/kaggle\", line 6, in <module>\n",
      "    from kaggle.cli import main\n",
      "  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/kaggle/__init__.py\", line 23, in <module>\n",
      "    api.authenticate()\n",
      "  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/kaggle/api/kaggle_api_extended.py\", line 149, in authenticate\n",
      "    self.config_file, self.config_dir))\n",
      "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/disks/user/anaconda3/bin/kaggle\", line 6, in <module>\n",
      "    from kaggle.cli import main\n",
      "  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/kaggle/__init__.py\", line 23, in <module>\n",
      "    api.authenticate()\n",
      "  File \"/mnt/disks/user/anaconda3/lib/python3.7/site-packages/kaggle/api/kaggle_api_extended.py\", line 149, in authenticate\n",
      "    self.config_file, self.config_dir))\n",
      "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n",
      "unzip:  cannot find or open flowers-recognition.zip, flowers-recognition.zip.zip or flowers-recognition.zip.ZIP.\n",
      "rm: cannot remove 'flowers-recognition.zip': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# For downloading the dataset\n",
    "# Go to Kaggle.com -> My Account -> Create new API Token -> Upload kaggle.json in Jupyter Home/CNN/\n",
    "!cp ../../../kaggle.json ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle config set -n proxy -v http://10.140.0.96:3128\n",
    "!kaggle datasets download -d alxmamaev/flowers-recognition\n",
    "!unzip flowers-recognition.zip\n",
    "!rm flowers-recognition.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (0.14.2)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from scikit-image) (1.0.2)\n",
      "Requirement already satisfied: networkx>=1.8 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from scikit-image) (2.2)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from scikit-image) (3.0.3)\n",
      "Requirement already satisfied: cloudpickle>=0.2.1 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from scikit-image) (0.8.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from scikit-image) (1.12.0)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from scikit-image) (5.4.1)\n",
      "Requirement already satisfied: dask[array]>=1.0.0 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from scikit-image) (1.1.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from PyWavelets>=0.4.0->scikit-image) (1.16.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from networkx>=1.8->scikit-image) (4.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.0.0->scikit-image) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.0.0->scikit-image) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.0.0->scikit-image) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from matplotlib>=2.0.0->scikit-image) (2.8.0)\n",
      "Requirement already satisfied: toolz>=0.7.3; extra == \"array\" in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from dask[array]>=1.0.0->scikit-image) (0.9.0)\n",
      "Requirement already satisfied: setuptools in /mnt/disks/user/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib>=2.0.0->scikit-image) (41.1.0)\n"
     ]
    }
   ],
   "source": [
    "# note that pip install skimage might throw an error, rather use this\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from skimage import io\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot n images using subplots\n",
    "def plot_image(images, captions=None, cmap=None ):\n",
    "    f, axes = plt.subplots(1, len(images), sharey=True)\n",
    "    f.set_figwidth(15)\n",
    "    for ax,image in zip(axes, images):\n",
    "        ax.imshow(image, cmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Image Augmentation & Preprocessing\n",
    "\n",
    "In the following section, we'll look at some common image preprocessing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding images and channels\n",
    "\n",
    "As these images are RGB images they would constitute three channels - one for each of the color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to your dataset\n",
    "DATASET_PATH = './flowers'\n",
    "flowers_cls = ['daisy', 'rose']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now use the `glob` module of python to <a href=\"https://en.wikipedia.org/wiki/Glob_(programming)\">glob</a> through the directory where the data is stored, i.e. to walk through the directory, subdirectories and the files. It uses regular expressions to access files having names matching some pattern. In our case, we want to access all the files in the path `flowers/rose/` and `flowers/daisy/`, so we'll just use the regex `*` (used as a 'wildcard' to catch everything). \n",
    "\n",
    "An example of how the glob module works is given below - you first join the base directory path with the subdirectory (e.g. `flowers/rose/`) and then `glob` through it to access all the individual files (images here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module glob:\n",
      "\n",
      "NAME\n",
      "    glob - Filename globbing utility.\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.7/library/glob\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "FUNCTIONS\n",
      "    escape(pathname)\n",
      "        Escape all special characters.\n",
      "    \n",
      "    glob(pathname, *, recursive=False)\n",
      "        Return a list of paths matching a pathname pattern.\n",
      "        \n",
      "        The pattern may contain simple shell-style wildcards a la\n",
      "        fnmatch. However, unlike fnmatch, filenames starting with a\n",
      "        dot are special cases that are not matched by '*' and '?'\n",
      "        patterns.\n",
      "        \n",
      "        If recursive is true, the pattern '**' will match any files and\n",
      "        zero or more directories and subdirectories.\n",
      "    \n",
      "    iglob(pathname, *, recursive=False)\n",
      "        Return an iterator which yields the paths matching a pathname pattern.\n",
      "        \n",
      "        The pattern may contain simple shell-style wildcards a la\n",
      "        fnmatch. However, unlike fnmatch, filenames starting with a\n",
      "        dot are special cases that are not matched by '*' and '?'\n",
      "        patterns.\n",
      "        \n",
      "        If recursive is true, the pattern '**' will match any files and\n",
      "        zero or more directories and subdirectories.\n",
      "\n",
      "DATA\n",
      "    __all__ = ['glob', 'iglob', 'escape']\n",
      "\n",
      "FILE\n",
      "    /mnt/disks/user/anaconda3/lib/python3.7/glob.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(glob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./flowers/rose/*\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bffa02707552>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# glob through the directory (returns a list of all file paths)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mflower_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflower_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflower_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# access an individual file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "## globbing example\n",
    "# help(glob)\n",
    "flower_path = os.path.join(DATASET_PATH, flowers_cls[1], '*')\n",
    "print(flower_path)\n",
    "\n",
    "# glob through the directory (returns a list of all file paths)\n",
    "flower_path = glob.glob(flower_path)\n",
    "print(flower_path[2]) # access an individual file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this block multiple times to look at some randomly chosen images of roses\n",
    "rand_index = random.randint(0, len(flower_path))\n",
    "image = io.imread(flower_path[rand_index])\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a sample image\n",
    "flower_path = os.path.join(DATASET_PATH, flowers_cls[1], '*')\n",
    "flower_path = glob.glob(flower_path)\n",
    "\n",
    "# access some element (a file) from the list\n",
    "image = io.imread(flower_path[729])\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the original image and the RGB channels\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, sharey=True)\n",
    "f.set_figwidth(15)\n",
    "ax1.imshow(image)\n",
    "\n",
    "# RGB channels\n",
    "ax2.imshow(image[:, : , 0])\n",
    "ax3.imshow(image[:, : , 1])\n",
    "ax4.imshow(image[:, : , 2])\n",
    "f.suptitle('Different Channels of Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding\n",
    "\n",
    "One of the simpler operations where we take all the pixels whose intensities are above a certain threshold, and convert them to ones; the pixels having value less than the threshold are converted to zero. This results in a *binary image*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bin_image will be a (240, 320) True/False array\n",
    "bin_image = image[:, :, 0] > 125\n",
    "plot_image([image, bin_image], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erosion, Dilation, Opening & Closing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Erosion** shrinks bright regions and enlarges dark regions. **Dilation** on the other hand is exact opposite side - it shrinks dark regions and enlarges the bright regions. \n",
    "\n",
    "**Opening** is erosion followed by dilation. Opening can remove small bright spots (i.e. “salt”) and connect small dark cracks. This tends to “open” up (dark) gaps between (bright) features.\n",
    "\n",
    "**Closing** is dilation followed by erosion. Closing can remove small dark spots (i.e. “pepper”) and connect small bright cracks. This tends to “close” up (dark) gaps between (bright) features.\n",
    "\n",
    "All these can be done using the `skimage.morphology` module. The basic idea is to have a **circular disk** of a certain size (3 below) move around the image and apply these transformations using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.morphology import binary_closing, binary_dilation, binary_erosion, binary_opening\n",
    "from skimage.morphology import selem\n",
    "\n",
    "# use a disk of radius 3\n",
    "selem = selem.disk(3)\n",
    "\n",
    "# oprning and closing\n",
    "open_img = binary_opening(bin_image, selem)\n",
    "close_img = binary_closing(bin_image, selem)\n",
    "\n",
    "# erosion and dilation\n",
    "eroded_img = binary_erosion(bin_image, selem)\n",
    "dilated_img = binary_dilation(bin_image, selem)\n",
    "\n",
    "plot_image([bin_image, open_img, close_img, eroded_img, dilated_img], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation is the most crucial step in the pre-processing part. There are multiple ways to normalise images which we will be talking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm1_image = image/255\n",
    "norm2_image = image - np.min(image)/np.max(image) - np.min(image)\n",
    "norm3_image = image - np.percentile(image,5)/ np.percentile(image,95) - np.percentile(image,5)\n",
    "\n",
    "plot_image([image, norm1_image, norm2_image, norm3_image], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple types of augmentations possible. The basic ones transform the original image using one of the following types of transformations:\n",
    "\n",
    "1. Linear transformations\n",
    "2. Affine transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from skimage import transform as tf\n",
    "\n",
    "# flip left-right, up-down\n",
    "image_flipr = np.fliplr(image)\n",
    "image_flipud = np.flipud(image)\n",
    "\n",
    "plot_image([image, image_flipr, image_flipud])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify x and y coordinates to be used for shifting (mid points)\n",
    "shift_x, shift_y = image.shape[0]/2, image.shape[1]/2\n",
    "\n",
    "# translation by certain units\n",
    "matrix_to_topleft = tf.SimilarityTransform(translation=[-shift_x, -shift_y])\n",
    "matrix_to_center = tf.SimilarityTransform(translation=[shift_x, shift_y])\n",
    "\n",
    "# rotation\n",
    "rot_transforms =  tf.AffineTransform(rotation=np.deg2rad(45))\n",
    "rot_matrix = matrix_to_topleft + rot_transforms + matrix_to_center\n",
    "rot_image = tf.warp(image, rot_matrix)\n",
    "\n",
    "# scaling \n",
    "scale_transforms = tf.AffineTransform(scale=(2, 2))\n",
    "scale_matrix = matrix_to_topleft + scale_transforms + matrix_to_center\n",
    "scale_image_zoom_out = tf.warp(image, scale_matrix)\n",
    "\n",
    "scale_transforms = tf.AffineTransform(scale=(0.5, 0.5))\n",
    "scale_matrix = matrix_to_topleft + scale_transforms + matrix_to_center\n",
    "scale_image_zoom_in = tf.warp(image, scale_matrix)\n",
    "\n",
    "# translation\n",
    "transaltion_transforms = tf.AffineTransform(translation=(50, 50))\n",
    "translated_image = tf.warp(image, transaltion_transforms)\n",
    "\n",
    "\n",
    "plot_image([image, rot_image, scale_image_zoom_out, scale_image_zoom_in, translated_image])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shear transforms\n",
    "shear_transforms = tf.AffineTransform(shear=np.deg2rad(45))\n",
    "shear_matrix = matrix_to_topleft + shear_transforms + matrix_to_center\n",
    "shear_image = tf.warp(image, shear_matrix)\n",
    "\n",
    "bright_jitter = image*0.999 + np.zeros_like(image)*0.001\n",
    "\n",
    "plot_image([image, shear_image, bright_jitter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Building\n",
    "\n",
    "Let's now build the network. We'll import the resnet architecture from the module `resnet.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import resnet\n",
    "\n",
    "# specify image size and channels\n",
    "img_channels = 3\n",
    "img_rows = 100\n",
    "img_cols = 100\n",
    "\n",
    "# number of classes\n",
    "nb_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator ###\n",
    "\n",
    "Let's now set up the **data generator**. The code below sets up a custom data generator which is slightly different than <a href=\"https://keras.io/preprocessing/image/\">the one that comes with the keras API</a>. The reason to use a custom generator is to be able to modify it according to the problem at hand (customizability). \n",
    "\n",
    "\n",
    "We won't be going through the entire code step-by-step in the lectures, though the code is explained below.\n",
    "\n",
    "To start with, we have the training data stored in $n$ directories (if there are $n$ classes). For a given batch size, we want to generate batches of data points and feed them to the model.\n",
    "\n",
    "\n",
    "The first `for` loop 'globs' through each of the classes (directories). For each class, it stores the path of each image in the list `paths`. In training mode, it subsets `paths` to contain the first 80% images; in validation mode it subsets the last 20%. In the special case of an ablation experiment, it simply subsets the first `ablation` images of each class.\n",
    "\n",
    "We store the paths of all the images (of all classes) in a combined list `self.list_IDs`. The dictionary `self.labels` contains the labels (as key:value pairs of `path: class_number (0/1)`).\n",
    "\n",
    "After the loop, we call the method `on_epoch_end()`, which creates an array `self.indexes` of length `self.list_IDs` and shuffles them (to shuffle all the data points at the end of each epoch).\n",
    "\n",
    "The `_getitem_` method uses the (shuffled) array `self.indexes` to select a `batch_size` number of entries (paths) from the path list `self.list_IDs`. \n",
    "\n",
    "Finally, the method `__data_generation` returns the batch of images as the pair X, y where X is of shape `(batch_size, height, width, channels)` and y is of shape `(batch size, )`. Note that `__data_generation` also does some preprocessing - it normalises the images (divides by 255) and crops the center 100 x 100 portion of the image. Thus, each image has the shape `(100, 100, num_channels)`. If any dimension (height or width) of an image less than 100 pixels, that image is deleted.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    \n",
    "    def __init__(self, mode='train', ablation=None, flowers_cls=['daisy', 'rose'], \n",
    "                 batch_size=32, dim=(100, 100), n_channels=3, shuffle=True):\n",
    "        \"\"\"\n",
    "        Initialise the data generator\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = {}\n",
    "        self.list_IDs = []\n",
    "        \n",
    "        # glob through directory of each class \n",
    "        for i, cls in enumerate(flowers_cls):\n",
    "            paths = glob.glob(os.path.join(DATASET_PATH, cls, '*'))\n",
    "            brk_point = int(len(paths)*0.8)\n",
    "            if mode == 'train':\n",
    "                paths = paths[:brk_point]\n",
    "            else:\n",
    "                paths = paths[brk_point:]\n",
    "            if ablation is not None:\n",
    "                paths = paths[:ablation]\n",
    "            self.list_IDs += paths\n",
    "            self.labels.update({p:i for p in paths})\n",
    "            \n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = len(flowers_cls)\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "        delete_rows = []\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            img = io.imread(ID)\n",
    "            img = img/255\n",
    "            if img.shape[0] > 100 and img.shape[1] > 100:\n",
    "                h, w, _ = img.shape\n",
    "                img = img[int(h/2)-50:int(h/2)+50, int(w/2)-50:int(w/2)+50, : ]\n",
    "            else:\n",
    "                delete_rows.append(i)\n",
    "                continue\n",
    "            \n",
    "            X[i,] = img\n",
    "          \n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "        \n",
    "        X = np.delete(X, delete_rows, axis=0)\n",
    "        y = np.delete(y, delete_rows, axis=0)\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Ablation Experiments \n",
    "\n",
    "Before training the net on the entire dataset, you should always try to first run some experiments to check whether the net is fitting on a small dataset or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking that the network is 'working'\n",
    "\n",
    "The first part of building a network is to get it to run on your dataset. Let's try fitting the net on only a few images and just one epoch. Note that since `ablation=100` is specified, 100 images of each class are used, so total number of batches is `np.floor(200/32)` = 6. \n",
    "\n",
    "Note that the `DataGenerator`  class 'inherits' from the `keras.utils.Sequence` class, so it has all the functionalities of the base `keras.utils.Sequence` class (such as the `model.fit_generator` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using resnet 18\n",
    "model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# create data generator objects in train and val mode\n",
    "# specify ablation=number of data points to train on\n",
    "training_generator = DataGenerator('train', ablation=100)\n",
    "validation_generator = DataGenerator('val', ablation=100)\n",
    "\n",
    "# fit: this will fit the net on 'ablation' samples, only 1 epoch\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting on the Training Data\n",
    "\n",
    "Let's now perform another important step which should be done before training the full-fledged model-  trying to **deliberately overfit the model** on a small dataset.\n",
    "\n",
    "We'll use ablation=100 (i.e. training on 100 images of each class), so it is still a very small dataset, and we will use 20 epochs. In each epoch, 200/32=6 batches will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet 18\n",
    "model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='SGD',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# generators\n",
    "training_generator = DataGenerator('train', ablation=100)\n",
    "validation_generator = DataGenerator('val', ablation=100)\n",
    "\n",
    "# fit\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the training accuracy increases consistently with each epoch. The validation accuracy also increases and then plateaus out - this is a sign of 'good fit', i.e. we know that the model is at least able to learn from a small dataset, so we can hope that it will be able to learn from the entire set as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "First let's make a list the hyper-parameters we want to tune:\n",
    "\n",
    "1. Learning Rate & Variation + Optimisers \n",
    "2. Augmentation Techniques\n",
    "\n",
    "The basic idea is to track the validation loss with increasing epochs for various values of a hyperparameter. \n",
    "\n",
    "#### Keras Callbacks ####\n",
    "\n",
    "Before you move ahead, let's discuss a bit about **callbacks**. Callbacks are basically actions that you want to perform at specific instances of training. For example, we want to perform the action of storing the loss at the end of every epoch (the instance here is the end of an epoch).\n",
    "\n",
    "\n",
    "Formally, a callback is simply a function (if you want to perform a single action), or a list of functions (if you want to perform multiple actions), which are to be executed at specific events (end of an epoch, start of every batch, when the accuracy plateaus out, etc.). Keras provides some very useful callback functionalities through the class `keras.callbacks.Callback`. \n",
    "\n",
    "Keras has many builtin callbacks (<a href=\"https://keras.io/callbacks/\">listed here</a>). The generic way to **create a custom callback in keras** is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic way to create custom callback\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we have created a custom callback to append the loss to a list at the end of every epoch. Note that `logs` is an attribute (a dictionary) of `keras.callbacks.Callback`, and we are using it to get the value of the key 'loss'. Some other keys of this dict are `acc`, `val_loss` etc.\n",
    "\n",
    "To tell the model that we want to use a callback, we create an object of `LossHistory` called `history` and pass it to `model.fit_generator` using `callbacks=[history]`. In this case, we only have one callback `history`, though you can pass multiple callback objects through this list (an example of multiple callbacks is in the section below - see the code block of `DecayLR()`).\n",
    "\n",
    "We highly recommend you to <a href=\"https://keras.io/callbacks/\">read the documentation of keras callbacks here</a>. For a gentler introduction to callbacks, you can read this <a href=\"https://machinelearningmastery.com/check-point-deep-learning-models-keras/\"> nice blog post by Jason Brownlee.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(keras.callbacks.Callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.callbacks import *\n",
    "\n",
    "# range of learning rates to tune\n",
    "hyper_parameters_for_lr = [0.1, 0.01, 0.001]\n",
    "\n",
    "# callback to append loss\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "\n",
    "# instantiate a LossHistory() object to store histories\n",
    "history = LossHistory()\n",
    "plot_data = {}\n",
    "\n",
    "# for each hyperparam: train the model and plot loss history\n",
    "for lr in hyper_parameters_for_lr:\n",
    "    print ('\\n\\n'+'=='*20 + '   Checking for LR={}  '.format(lr) + '=='*20 )\n",
    "    sgd = optimizers.SGD(lr=lr, clipnorm=1.)\n",
    "    \n",
    "    # model and generators\n",
    "    model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer= sgd,\n",
    "                  metrics=['accuracy'])\n",
    "    training_generator = DataGenerator('train', ablation=100)\n",
    "    validation_generator = DataGenerator('val', ablation=100)\n",
    "    model.fit_generator(generator=training_generator,\n",
    "                        validation_data=validation_generator,\n",
    "                        epochs=3, callbacks=[history])\n",
    "    \n",
    "    # plot loss history\n",
    "    plot_data[lr] = history.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot loss history for each value of hyperparameter\n",
    "f, axes = plt.subplots(1, 3, sharey=True)\n",
    "f.set_figwidth(15)\n",
    "\n",
    "plt.setp(axes, xticks=np.arange(0, len(plot_data[0.01]), 1)+1)\n",
    "\n",
    "for i, lr in enumerate(plot_data.keys()):\n",
    "    axes[i].plot(np.arange(len(plot_data[lr]))+1, plot_data[lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above show that a learning rate of 0.1 is the best, though using such a high learning rate for the entire training is usually not a good idea. Thus, we should use **learning rate decay** - starting from a high learning rate and decaying it with every epoch.\n",
    "\n",
    "We use another **custom callback** (`DecayLR`) to decay the learning rate at the end of every epoch. The decay rate is specified as 0.5 ^ epoch. Also, note that this time we are telling the model to **use two callbacks** (passed as a list `callbacks=[history, decay]` to `model.fit_generator`).\n",
    "\n",
    "\n",
    "Although we have used out own custom decay implementation here, you can use the ones built into <a href=\"https://keras.io/optimizers/\">keras optimisers</a> (using the `decay` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate decay\n",
    "class DecayLR(keras.callbacks.Callback):\n",
    "    def __init__(self, base_lr=0.001, decay_epoch=1):\n",
    "        super(DecayLR, self).__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.decay_epoch = decay_epoch \n",
    "        self.lr_history = []\n",
    "        \n",
    "    # set lr on_train_begin\n",
    "    def on_train_begin(self, logs={}):\n",
    "        K.set_value(self.model.optimizer.lr, self.base_lr)\n",
    "\n",
    "    # change learning rate at the end of epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        new_lr = self.base_lr * (0.5 ** (epoch // self.decay_epoch))\n",
    "        self.lr_history.append(K.get_value(self.model.optimizer.lr))\n",
    "        K.set_value(self.model.optimizer.lr, new_lr)\n",
    "\n",
    "# to store loss history\n",
    "history = LossHistory()\n",
    "plot_data = {}\n",
    "\n",
    "# start with lr=0.1\n",
    "decay = DecayLR(base_lr=0.1)\n",
    "\n",
    "# model\n",
    "sgd = optimizers.SGD()\n",
    "model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\n",
    "model.compile(loss='categorical_crossentropy',optimizer= sgd,\n",
    "              metrics=['accuracy'])\n",
    "training_generator = DataGenerator('train', ablation=100)\n",
    "validation_generator = DataGenerator('val', ablation=100)\n",
    "\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=3, callbacks=[history, decay])\n",
    "\n",
    "plot_data[lr] = decay.lr_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(decay.lr_history)), decay.lr_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation Techniques\n",
    "\n",
    "Let's now write some code to implement data augmentation. Augmentation is usually done with data generators, i.e. the augmented data is generated batch-wise, on the fly. \n",
    "\n",
    "You can either use the built-in keras `ImageDataGenerator` or write your own data generator (for some custom features etc if you want). The two cells below show how to implement these respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras data generator\n",
    "# help(ImageDataGenerator)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to implement your own customized data generator (with augmentation), you can add the augmentation step easily to the `DataGenerator` class created above. The only change is that we stack the augmented images to the X, y arrays (as done in the last section of the code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# data generator with augmentation\n",
    "class AugmentedDataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, mode='train', ablation=None, flowers_cls=['daisy', 'rose'], \n",
    "                 batch_size=32, dim=(100, 100), n_channels=3, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = {}\n",
    "        self.list_IDs = []\n",
    "        self.mode = mode\n",
    "        \n",
    "        for i, cls in enumerate(flowers_cls):\n",
    "            paths = glob.glob(os.path.join(DATASET_PATH, cls, '*'))\n",
    "            brk_point = int(len(paths)*0.8)\n",
    "            if self.mode == 'train':\n",
    "                paths = paths[:brk_point]\n",
    "            else:\n",
    "                paths = paths[brk_point:]\n",
    "            if ablation is not None:\n",
    "                paths = paths[:ablation]\n",
    "            self.list_IDs += paths\n",
    "            self.labels.update({p:i for p in paths})\n",
    "        \n",
    "            \n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = len(flowers_cls)\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "        delete_rows = []\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            img = io.imread(ID)\n",
    "            img = img/255\n",
    "            if img.shape[0] > 100 and img.shape[1] > 100:\n",
    "                h, w, _ = img.shape\n",
    "                img = img[int(h/2)-50:int(h/2)+50, int(w/2)-50:int(w/2)+50, : ]\n",
    "            else:\n",
    "                delete_rows.append(i)\n",
    "                continue\n",
    "            \n",
    "            X[i,] = img\n",
    "          \n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "        \n",
    "        X = np.delete(X, delete_rows, axis=0)\n",
    "        y = np.delete(y, delete_rows, axis=0)\n",
    "        \n",
    "        # data augmentation\n",
    "        if self.mode == 'train':\n",
    "            aug_x = np.stack([datagen.random_transform(img) for img in X])\n",
    "            X = np.concatenate([X, aug_x])\n",
    "            y = np.concatenate([y, y])\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics to optimise\n",
    "\n",
    "AUC is often a better metric than accuracy. So instead of optimising for accuracy, let's monitor AUC and choose the best model based on AUC on validaton data. We'll use the callbacks `on_train_begin` and `on_epoch_end` to initialise  (at the start of each epoch) and store the AUC (at the end of epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class roc_callback(Callback):\n",
    "    \n",
    "    def on_train_begin(self, logs={}):\n",
    "        logs['val_auc'] = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_p = []\n",
    "        y_v = []\n",
    "        for i in range(len(validation_generator)):\n",
    "            x_val, y_val = validation_generator[i]\n",
    "            y_pred = self.model.predict(x_val)\n",
    "            y_p.append(y_pred)\n",
    "            y_v.append(y_val)\n",
    "        y_p = np.concatenate(y_p)\n",
    "        y_v = np.concatenate(y_v)\n",
    "        roc_auc = roc_auc_score(y_v, y_p)\n",
    "        print ('\\nVal AUC for epoch{}: {}'.format(epoch, roc_auc))\n",
    "        logs['val_auc'] = roc_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Run\n",
    "\n",
    "Let's now train the final model. Note that we will keep saving the best model's weights at `models/best_models.hdf5`, so you will need to create a directory `models`. Note that model weights are usually saved in hdf5 files.\n",
    "\n",
    "**Saving the best model** is done using the callback functionality that comes with `ModelCheckpoint`. We basically specify the `filepath` where the model weights are to be saved, ` monitor='val_auc'` specifies that you are choosing the best model based on validation accuracy, `save_best_only=True` saves only the best weights, and `mode='max'` specifies that the validation accuracy is to be maximised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = resnet.ResnetBuilder.build_resnet_18((img_channels, img_rows, img_cols), nb_classes)\n",
    "model.compile(loss='categorical_crossentropy',optimizer= sgd,\n",
    "              metrics=['accuracy'])\n",
    "training_generator = AugmentedDataGenerator('train', ablation=32)\n",
    "validation_generator = AugmentedDataGenerator('val', ablation=32)\n",
    "\n",
    "# checkpoint \n",
    "filepath = 'models/best_model.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\n",
    "auc_logger = roc_callback()\n",
    "\n",
    "# fit \n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    epochs=3, callbacks=[auc_logger, history, decay, checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w, _ = image.shape\n",
    "img = image[int(h/2)-50:int(h/2)+50, int(w/2)-50:int(w/2)+50, : ]\n",
    "\n",
    "model.predict(img[np.newaxis,: ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_grads(model, input_array):\n",
    "    grads_fn = K.gradients(model.output, model.input)[0]\n",
    "    compute_fn = K.function([model.input, K.learning_phase()], [grads_fn])\n",
    "    return compute_fn([np.array([input_array]), 0])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = _compute_grads(model, img)\n",
    "grad_normed = (grad - np.min(grad))/(np.max(grad) - np.min(grad))\n",
    "grad_normed *= 255\n",
    "plt.imshow(grad_normed.astype('uint8'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
