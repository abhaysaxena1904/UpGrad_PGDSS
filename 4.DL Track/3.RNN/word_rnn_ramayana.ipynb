{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation using RNN - Word Level\n",
    "\n",
    "To generate text using RNN, we need a to convert raw text to a supervised learning problem format.\n",
    "\n",
    "Take, for example, the following corpus:\n",
    "\n",
    "\"Her brother shook his head incredulously. He was not aware of the situation at all.\"\n",
    "\n",
    "First we need to divide the data into tabular format containing input (X) and output (y) sequences. In case of a character level model, the X and y will look like this:\n",
    "\n",
    "|      X                |  Y      |\n",
    "|-----------------------|---------|\n",
    "|    < word1 >< word2 > | < word3 > |\n",
    "|    Her brother        |  shook  |\n",
    "|    brother shook      |  his    |\n",
    "|    shook his          |  head   |\n",
    "|    his head           | incredulously |\n",
    "|    head incredulously |    .    |\n",
    "|    ..                 |    .    |\n",
    "|    situation at       |  all    |\n",
    "|    at all             |    .    |\n",
    "\n",
    "Note that in the above problem, the sequence length of **X is two words** and that of **y is one word**. Hence, this is a many-to-one architecture. We can, however, change the number of input words to any number depending on the problem.\n",
    "\n",
    "A model is trained on such data. To generate text, we simply give the model any two words using which it predicts the next word. Then it appends the predicted word to the input sequence (to the extreme right of the sequence) and discards the first word (word on extreme left of the sequence). Then it predicts again using the new sequence and the cycle continues until a fix number of iterations. An example is shown below:\n",
    "\n",
    "Seed text: \"Did I\"\n",
    "\n",
    "|      X                                            |  Y                       |\n",
    "|---------------------------------------------------|--------------------------|\n",
    "|                        Did I                      |    < predicted word 1 >  |\n",
    "|               I < predicted word 1 >              |    < predicted word 2 >  |\n",
    "|       < predicted word 1 > < predicted word 2 >   |    < predicted word 3 >  |\n",
    "|       < predicted word 2 > < predicted word 3 >   |    < predicted word 4 >  |\n",
    "|                      ...                          |            ...           | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "1. Preprocess data\n",
    "2. Build LSTM model\n",
    "3. Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "n5R1APadwANc",
    "outputId": "3553d1b6-cb0c-4adf-efac-333da6e7f547",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXzkqw1ipoPg"
   },
   "outputs": [],
   "source": [
    "# download ebook\n",
    "url = \"https://www.gutenberg.org/files/24869/24869-0.txt\"\n",
    "book = requests.get(url)\n",
    "data = book.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg EBook of The Ramayana\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with almost no\r\n",
      "restrictions whatsoever. You may copy it, give it away or re-use it under\r\n",
      "the terms of the Project Gutenberg License included with this eBook or\r\n",
      "online at http://www.gutenberg.org/license\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Title: The Ramayana\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "Release Date: March 18, 2008 [Ebook #24869]\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "Character set encoding: UTF-8\r\n",
      "\r\n",
      "\r\n",
      "***START OF THE PROJECT GUTENBERG EBOOK THE\n"
     ]
    }
   ],
   "source": [
    "# let's look at the text\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19177\n"
     ]
    }
   ],
   "source": [
    "# subset the book from the first chapter, that ism INVOCATION - everything before first chapter is irrelevant data\n",
    "start_index = re.search(\"invocation.\\(1\\)\", data, re.I)\n",
    "print(start_index.start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how does the text look like\n",
    "data = data[start_index.start():]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INVOCATION.(1)\n",
      "\n",
      "\n",
      "Praise to VÃ¡lmÃ­ki,(2)bird of charming song,(3)\n",
      "  Who mounts on Poesyâs sublimest spray,\n",
      "And sweetly sings with accent clear and strong\n",
      "  RÃ¡ma, aye RÃ¡ma, in his deathless lay.\n",
      "\n",
      "Where breathes the man can listen to the strain\n",
      "  That flows in music from VÃ¡lmÃ­kiâs tongue,\n",
      "Nor feel his feet the path of bliss attain\n",
      "  When RÃ¡maâs glory by the saint is sung!\n",
      "\n",
      "The stream RÃ¡mÃ¡yan leaves its sacred fount\n",
      "  The whole wide world from sin and stain to free.(4)\n",
      "T\n"
     ]
    }
   ],
   "source": [
    "# let's look at the text\n",
    "print(data[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91975\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to clean text data\n",
    "def clean_document(document, char_filter = r\"[^\\w]\"):\n",
    "    '''\n",
    "    input:\n",
    "    document          :  string\n",
    "    char_filter       :  regex pattern - removes those characters from the text that match the pattern\n",
    "\n",
    "    output: clean document\n",
    "    '''\n",
    "    \n",
    "    # convert words to lower case\n",
    "    document = document.lower()\n",
    "\n",
    "    # tokenise words\n",
    "    words = word_tokenize(document)\n",
    "\n",
    "    # strip whitespace from all words\n",
    "    words = [word.strip() for word in words]\n",
    "\n",
    "    # join back words to get document\n",
    "    document = \" \".join(words)\n",
    "\n",
    "    # remove unwanted characters\n",
    "    document = regex.sub(char_filter, \" \", document)\n",
    "\n",
    "    # replace multiple whitespaces with single whitespace\n",
    "    document = regex.sub(r\"\\s+\", \" \", document)\n",
    "\n",
    "    # strip whitespace from document\n",
    "    document = document.strip()\n",
    "\n",
    "    return document\n",
    "\n",
    "data = clean_document(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in document: 427304\n"
     ]
    }
   ],
   "source": [
    "# length of text\n",
    "words = word_tokenize(data)\n",
    "print(\"Number of words in document: {}\".format(len(words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert characters to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PBkfQnPGqV_4",
    "outputId": "5d32cb08-61b0-4695-a4fe-4508b4b3e898"
   },
   "outputs": [],
   "source": [
    "# use Keras' Tokenizer() function to encode text to integers\n",
    "word_tokeniser = Tokenizer()\n",
    "word_tokeniser.fit_on_texts([data])\n",
    "encoded_words = word_tokeniser.texts_to_sequences([data])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 18415\n"
     ]
    }
   ],
   "source": [
    "# check the size of the vocabulary\n",
    "VOCABULARY_SIZE = len(word_tokeniser.word_index) + 1\n",
    "print('Vocabulary Size: {}'.format(VOCABULARY_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data in X and y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequences\n",
    "\n",
    "In each training sample, X will have a sequence of 5 words and y will have the sixth word. In other words, this means that use previous five words of a sequence to predict next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hwJLfNn3qX56",
    "outputId": "bef2447b-77d1-43f3-e64b-a098ed2e87e1"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "MAX_SEQ_LENGTH = 5  # X will have five words, y will have the sixth word\n",
    "\n",
    "for i in range(MAX_SEQ_LENGTH, len(encoded_words)):\n",
    "    sequence = encoded_words[i-MAX_SEQ_LENGTH:i+1]\n",
    "    sequences.append(sequence)\n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training samples: 427133\n",
      "\n",
      "Sample sequences: \n",
      "[[7128  842  887    4   54 1095]\n",
      " [ 842  887    4   54 1095 1353]\n",
      " [ 887    4   54 1095 1353 1999]]\n"
     ]
    }
   ],
   "source": [
    "print('Total number of training samples: {}'.format(len(sequences)))\n",
    "print('\\nSample sequences: \\n{}'.format(sequences[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3baVAiOqZpj"
   },
   "outputs": [],
   "source": [
    "# divide the sequence into X and y\n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = sequences[:,:-1]  # assign all but last words of a sequence to X\n",
    "y = sequences[:,-1]   # assign last word of each sequence to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input of the first data point: [7128  842  887    4   54] \n",
      "\n",
      "Output of the first data point: [ 1095 ]\n"
     ]
    }
   ],
   "source": [
    "# Look at the first training example\n",
    "print(\"Input of the first data point:\", X[0], \"\\n\")\n",
    "print(\"Output of the first data point: [\", y[0], \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(427133,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 29.3 GiB for an array with shape (427133, 18415) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-d5c984dab5f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVOCABULARY_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 29.3 GiB for an array with shape (427133, 18415) and data type float32"
     ]
    }
   ],
   "source": [
    "y = to_categorical(y, num_classes=VOCABULARY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HX_Npp_cVWyW",
    "outputId": "af76e422-420e-4b83-cd42-6b1dda6011ac"
   },
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 410241 sequences (data points) in total.\n",
    "\n",
    "Remember that to use an RNN data has to be of the shape (#samples, #timesteps, #features)\n",
    "\n",
    "In X, the third dimension, that is, number of features is missing because we're going to use the Keras' Embedding Layer. Hence we don't need to explicitly reshape the data to incorporate the third dimension. That will be done automatically by Keras.\n",
    "\n",
    "In y, the second dimension is missing, that is, the number of timesteps because y is not a sequence, it's just a single word. The number of features are represented by a one-hot encoded vector whose length is the VOCABULARY_SIZE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH, padding='pre')\n",
    "print('Input sequence length: {}'.format(MAX_SEQ_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model architecture\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "model.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length = MAX_SEQ_LENGTH))\n",
    "\n",
    "# lstm layer 1\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "# lstm layer 2\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(VOCABULARY_SIZE, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "# summarize defined model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5134
    },
    "colab_type": "code",
    "id": "cvY3R74rqbOu",
    "outputId": "68541bf7-bc2f-4fa6-c6c9-8787a7da8e51"
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "model.fit(X, y, epochs=10, verbose=2, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word embeddings to represent the input words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec download link (Size ~ 1.5GB): https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "path = 'storage/word-embeddings/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "# load word2vec using the following function present in the gensim library\n",
    "word2vec = KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign word vectors from word2vec model\n",
    "\n",
    "EMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\n",
    "VOCABULARY_SIZE = len(word_tokeniser.word_index) + 1\n",
    "\n",
    "# create an empty embedding matix\n",
    "embedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n",
    "\n",
    "# create a word to index dictionary mapping\n",
    "word2id = word_tokeniser.word_index\n",
    "\n",
    "# copy vectors from word2vec model to the words present in corpus\n",
    "for word, index in word2id.items():\n",
    "    try:\n",
    "        embedding_weights[index, :] = word2vec[word]\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check embedding dimension\n",
    "print(\"Embeddings shape: {}\".format(embedding_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model architecture\n",
    "\n",
    "model_wv = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "model_wv.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length = MAX_SEQ_LENGTH,\n",
    "                    weights = [embedding_weights], trainable=True))\n",
    "\n",
    "# lstm layer 1\n",
    "model_wv.add(LSTM(128, return_sequences=True))\n",
    "\n",
    "# lstm layer 2\n",
    "# when using multiple LSTM layers, set return_sequences to True at the previous layer\n",
    "# because the current layer expects a sequential intput rather than a single input\n",
    "model_wv.add(LSTM(128))\n",
    "\n",
    "# output layer\n",
    "model_wv.add(Dense(VOCABULARY_SIZE, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile network\n",
    "model_wv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  \n",
    "# summarize defined model\n",
    "model_wv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 5134
    },
    "colab_type": "code",
    "id": "cvY3R74rqbOu",
    "outputId": "68541bf7-bc2f-4fa6-c6c9-8787a7da8e51"
   },
   "outputs": [],
   "source": [
    "# fit network\n",
    "model_wv.fit(X, y, epochs=10, batch_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NiH8h5oqkX5"
   },
   "outputs": [],
   "source": [
    "# generate a sequence from a language model\n",
    "def generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed, n_words):\n",
    "    \n",
    "    text = seed\n",
    "    \n",
    "    # generate n_words\n",
    "    for _ in range(n_words):\n",
    "        \n",
    "        # encode text as integers\n",
    "        encoded_words = word_tokeniser.texts_to_sequences([text])[0]\n",
    "        \n",
    "        # pad sequences\n",
    "        padded_words = pad_sequences([encoded_words], maxlen=MAX_SEQ_LENGTH, padding='pre')\n",
    "        \n",
    "        # predict next word\n",
    "        prediction = model.predict_classes(padded_words, verbose=0)\n",
    "        \n",
    "        # convert predicted index to its word\n",
    "        next_word = \"\"\n",
    "        for word, i in word_tokeniser.word_index.items():\n",
    "            if i == prediction:\n",
    "                next_word = word\n",
    "                break\n",
    "        \n",
    "        # append predicted word to text\n",
    "        text += \" \" + next_word\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some text generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [],
   "source": [
    "# text generation using first model - model without word embeddings\n",
    "seed_text = \"rama never told anyone about\"\n",
    "num_words = 100\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [],
   "source": [
    "# text generation using second model - model with word embeddings\n",
    "seed_text = \"rama never told anyone about\"\n",
    "num_words = 100\n",
    "print(generate_words(model_wv, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [],
   "source": [
    "# text generation using first model - model without word embeddings\n",
    "seed_text = \"how are you doing\"\n",
    "num_words = 100\n",
    "print(generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "gmhTo5KgjPFk",
    "outputId": "761a2128-4b44-41fa-e42d-7fe1f79cbe2f"
   },
   "outputs": [],
   "source": [
    "# text generation using second model - model with word embeddings\n",
    "seed_text = \"how are you doing\"\n",
    "num_words = 100\n",
    "print(generate_words(model_wv, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "word_rnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
